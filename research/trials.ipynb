{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal_Projects\\All_Project\\Question-Answering-Application-Using-Bert\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "weight_path = \"kaporter/bert-base-uncased-finetuned-squad\"\n",
    "# loading tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(weight_path)\n",
    "#loading the model\n",
    "model = BertForQuestionAnswering.from_pretrained(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take an example\n",
    "question = \"How many parameters does BERT-large have?\"\n",
    "\n",
    "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "\n",
    "answer = \"340M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets generate token_ids using tokenizer\n",
    "question = \"How many parameters does BERT-large have?\"\n",
    "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "\n",
    "input_ids = tokenizer.encode(question, context)\n",
    "print (f'We have about {len(input_ids)} tokens generated')\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\" \")\n",
    "print('Some examples of token-input_id pairs:')\n",
    "\n",
    "for i, (token,inp_id) in enumerate(zip(tokens,input_ids)):\n",
    "    print(token,\":\",inp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets pass our input through model and sees the output.\n",
    "\n",
    "# Run our example through the model.\n",
    "out = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                token_type_ids=torch.tensor([token_type_ids]))\n",
    "\n",
    "start_logits,end_logits = out['start_logits'],out['end_logits']\n",
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "answer_start = torch.argmax(start_logits)\n",
    "answer_end = torch.argmax(end_logits)\n",
    "\n",
    "ans = ''.join(tokens[answer_start:answer_end])\n",
    "print('Predicted answer:', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

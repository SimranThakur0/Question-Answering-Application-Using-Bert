{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal_Projects\\All_Project\\Question-Answering-Application-Using-Bert\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "weight_path = \"kaporter/bert-base-uncased-finetuned-squad\"\n",
    "# loading tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(weight_path)\n",
    "#loading the model\n",
    "model = BertForQuestionAnswering.from_pretrained(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take an example\n",
    "question = \"How many parameters does BERT-large have?\"\n",
    "\n",
    "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "\n",
    "answer = \"340M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have about 70 tokens generated\n",
      " \n",
      "Some examples of token-input_id pairs:\n",
      "[CLS] : 101\n",
      "how : 2129\n",
      "many : 2116\n",
      "parameters : 11709\n",
      "does : 2515\n",
      "bert : 14324\n",
      "- : 1011\n",
      "large : 2312\n",
      "have : 2031\n",
      "? : 1029\n",
      "[SEP] : 102\n",
      "bert : 14324\n",
      "- : 1011\n",
      "large : 2312\n",
      "is : 2003\n",
      "really : 2428\n",
      "big : 2502\n",
      ". : 1012\n",
      ". : 1012\n",
      ". : 1012\n",
      "it : 2009\n",
      "has : 2038\n",
      "24 : 2484\n",
      "- : 1011\n",
      "layers : 9014\n",
      "and : 1998\n",
      "an : 2019\n",
      "em : 7861\n",
      "##bed : 8270\n",
      "##ding : 4667\n",
      "size : 2946\n",
      "of : 1997\n",
      "1 : 1015\n",
      ", : 1010\n",
      "02 : 6185\n",
      "##4 : 2549\n",
      ", : 1010\n",
      "for : 2005\n",
      "a : 1037\n",
      "total : 2561\n",
      "of : 1997\n",
      "340 : 16029\n",
      "##m : 2213\n",
      "parameters : 11709\n",
      "! : 999\n",
      "altogether : 10462\n",
      "it : 2009\n",
      "is : 2003\n",
      "1 : 1015\n",
      ". : 1012\n",
      "34 : 4090\n",
      "##gb : 18259\n",
      ", : 1010\n",
      "so : 2061\n",
      "expect : 5987\n",
      "it : 2009\n",
      "to : 2000\n",
      "take : 2202\n",
      "a : 1037\n",
      "couple : 3232\n",
      "minutes : 2781\n",
      "to : 2000\n",
      "download : 8816\n",
      "to : 2000\n",
      "your : 2115\n",
      "cola : 15270\n",
      "##b : 2497\n",
      "instance : 6013\n",
      ". : 1012\n",
      "[SEP] : 102\n"
     ]
    }
   ],
   "source": [
    "#Now lets generate token_ids using tokenizer\n",
    "question = \"How many parameters does BERT-large have?\"\n",
    "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "\n",
    "input_ids = tokenizer.encode(question, context)\n",
    "print (f'We have about {len(input_ids)} tokens generated')\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\" \")\n",
    "print('Some examples of token-input_id pairs:')\n",
    "\n",
    "for i, (token,inp_id) in enumerate(zip(tokens,input_ids)):\n",
    "    print(token,\":\",inp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sep_idx = tokens.index('[SEP]')\n",
    "\n",
    "# we will provide including [SEP] token which seperates question from context and 1 for rest.\n",
    "token_type_ids = [0 for i in range(sep_idx+1)] + [1 for i in range(sep_idx+1,len(tokens))]\n",
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: 340\n"
     ]
    }
   ],
   "source": [
    "#Now lets pass our input through model and sees the output.\n",
    "\n",
    "# Run our example through the model.\n",
    "out = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                token_type_ids=torch.tensor([token_type_ids]))\n",
    "\n",
    "start_logits,end_logits = out['start_logits'],out['end_logits']\n",
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "answer_start = torch.argmax(start_logits)\n",
    "answer_end = torch.argmax(end_logits)\n",
    "\n",
    "ans = ''.join(tokens[answer_start:answer_end])\n",
    "print('Predicted answer:', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
